{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bcd74cf",
   "metadata": {},
   "source": [
    "# NDVI-Stats: Fast & Interpretable Land Cover Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae2148",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9917f08",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e56dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('hacktrain.csv')\n",
    "test_df = pd.read_csv('hacktest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c9113",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9355483",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629c24f",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdf40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee8f53",
   "metadata": {},
   "source": [
    "## 5. Model Training (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946da4f",
   "metadata": {},
   "source": [
    "## 6. Results & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the missing values\n",
    "missing_counts = train_df.isnull().sum()\n",
    "print(\"Missing values in training data:\")\n",
    "print(missing_counts[missing_counts > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893d388",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the unique classes\n",
    "print(\"Unique classes in training set:\", train_df['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf075005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how the classes are distributed in the dataset\n",
    "train_df['class'].value_counts().plot(kind='bar', title=\"Class Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d72c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of NDVI time-series columns\n",
    "ndvi_cols = [col for col in train_df.columns if '_N' in col]\n",
    "print(\"Total NDVI time-series columns:\", len(ndvi_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04229b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing NDVI values with median (can be changed to mean or interpolation)\n",
    "train_df[ndvi_cols] = train_df[ndvi_cols].fillna(train_df[ndvi_cols].median())\n",
    "test_df[ndvi_cols] = test_df[ndvi_cols].fillna(test_df[ndvi_cols].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f4000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature functions\n",
    "def extract_features(df):\n",
    "    df['ndvi_mean'] = df[ndvi_cols].mean(axis=1)\n",
    "    df['ndvi_std'] = df[ndvi_cols].std(axis=1)\n",
    "    df['ndvi_min'] = df[ndvi_cols].min(axis=1)\n",
    "    df['ndvi_max'] = df[ndvi_cols].max(axis=1)\n",
    "    df['ndvi_range'] = df['ndvi_max'] - df['ndvi_min']\n",
    "    df['ndvi_slope'] = df[ndvi_cols].apply(lambda row: row.values[-1] - row.values[0], axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply on both train and test\n",
    "train_df = extract_features(train_df)\n",
    "test_df = extract_features(test_df)\n",
    "\n",
    "# Final features to train on\n",
    "feature_cols = ['ndvi_mean', 'ndvi_std', 'ndvi_min', 'ndvi_max', 'ndvi_range', 'ndvi_slope']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot NDVI Time-Series Trends by class\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Columns representing NDVI time-series\n",
    "ndvi_cols = [col for col in train_df.columns if '_N' in col]\n",
    "\n",
    "# Group by class and compute mean NDVI over time\n",
    "ndvi_by_class = train_df.groupby('class')[ndvi_cols].mean().T\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "for land_type in ndvi_by_class.columns:\n",
    "    plt.plot(ndvi_by_class.index, ndvi_by_class[land_type], label=land_type)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Date (NDVI Columns)\")\n",
    "plt.ylabel(\"Average NDVI Value\")\n",
    "plt.title(\"NDVI Trends Over Time by Land Cover Type\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_df['label'] = le.fit_transform(train_df['class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def advanced_features(df):\n",
    "    df['ndvi_skew'] = df[ndvi_cols].apply(lambda row: skew(row, nan_policy='omit'), axis=1)\n",
    "    df['ndvi_kurtosis'] = df[ndvi_cols].apply(lambda row: kurtosis(row, nan_policy='omit'), axis=1)\n",
    "\n",
    "    # Split into 3 periods (early, mid, late)\n",
    "    thirds = len(ndvi_cols) // 3\n",
    "    df['ndvi_early_avg'] = df[ndvi_cols[:thirds]].mean(axis=1)\n",
    "    df['ndvi_mid_avg'] = df[ndvi_cols[thirds:2*thirds]].mean(axis=1)\n",
    "    df['ndvi_late_avg'] = df[ndvi_cols[2*thirds:]].mean(axis=1)\n",
    "\n",
    "    # NDVI above threshold (e.g., healthy vegetation > 0.4)\n",
    "    df['ndvi_above_0.4'] = df[ndvi_cols].gt(0.4).sum(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply on both train and test\n",
    "train_df = advanced_features(train_df)\n",
    "test_df = advanced_features(test_df)\n",
    "\n",
    "# Update feature list\n",
    "feature_cols += ['ndvi_skew', 'ndvi_kurtosis', 'ndvi_early_avg', 'ndvi_mid_avg', 'ndvi_late_avg', 'ndvi_above_0.4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[feature_cols]\n",
    "y = train_df['label']\n",
    "\n",
    "\n",
    "# Same train-test split and model as before...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8dd484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Feature and label\n",
    "X = train_df[feature_cols]\n",
    "y = train_df['label']\n",
    "\n",
    "# Split for local validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Build pipeline: Standardize + Logistic Regression\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validation accuracy\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e05147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on the test data\n",
    "X_test = test_df[feature_cols]\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "# Convert numerical labels back to original class names\n",
    "test_preds_labels = le.inverse_transform(test_preds)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'class': test_preds_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc8da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('submission.csv')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
